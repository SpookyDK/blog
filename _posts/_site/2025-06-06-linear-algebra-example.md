
---
layout: post
title: "Linear Algebra Example"
date: 2025-06-10
categories: [blog]
---
<!-- KaTeX CSS og JS -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
  onload="renderMathInElement(document.body, {
    delimiters: [
      {left: '$$', right: '$$', display: true},
      {left: '$', right: '$', display: false}
    ]
  });"></script>
# üìò Line√¶r Algebra ‚Äì Eksamen 2025

## 1. Hvordan multiplicerer man to matricer? Hvilke st√∏rrelser skal de to matricer A og B have, s√• at deres produkt AB er defineret? Lille eksempel p√• det, fx 3√ó2-matrix gange to-dimensionelt vektor.

To matricer

$$
A \in \mathbb{R}^{m \times n} \quad \text{og} \quad B \in \mathbb{R}^{n \times p}
$$

kan multipliceres, hvis **antallet af s√∏jler i $A$** er lig med **antallet af r√¶kker i $B$**. Produktet

$$
AB \in \mathbb{R}^{m \times p}
$$

bliver da en matrix i $\mathbb{R}^{m \times p}$.

**Eksempel:**

$$
A = \begin{bmatrix} 1 & 2 \\ 3 & 4 \\ 5 & 6 \end{bmatrix}, \quad
x = \begin{bmatrix} 7 \\ 8 \end{bmatrix}
\Rightarrow
Ax = \begin{bmatrix} 1\cdot7 + 2\cdot8 \\ 3\cdot7 + 4\cdot8 \\ 5\cdot7 + 6\cdot8 \end{bmatrix} = \begin{bmatrix} 23 \\ 53 \\ 83 \end{bmatrix}
$$

---

## 2. G√¶lder altid AB = BA, hvis A og B begge er n√ón-matricer? (Nej, oftest ikke!)

Nej, matrixmultiplikation er **ikke kommutativ**. Dvs. generelt g√¶lder:

$$
AB \neq BA
$$

**Undtagelser:** Det kan dog g√¶lde, hvis:
- \( A = I \) (identitetsmatrix)
- \( A \) og \( B \) er diagonale og kommuterer
- \( A \) og \( B \) er potenser af samme matrix

---

## 3. Hvad sker, n√•r man ganger matricen A fra venstre eller h√∏jre med identitetsmatricen I af den tilpasse st√∏rrelse? (Ingenting, man f√•r bare igen A.)

Hvis 
$$
A = \mathbb{R}^{m \times n} \quad \text{og} \quad I
$$
har passende st√∏rrelse, s√• g√¶lder:
$$
IA = A \quad \text{og} \quad AI = A
$$

---

## 4. Hvor mange l√∏sninger kan et line√¶rt ligningssystem Ax = b i princippet have?

Et line√¶rt system kan have:
- **Ingen l√∏sning** (inkonsistent)
- **√ân entydig l√∏sning**
- **Uendeligt mange l√∏sninger**

---

## 5. Hvordan kan man unders√∏ge, om Ax = b har l√∏sninger og i givet fald finde dem?

Brug **Gauss-elimination** til at omskrive systemet til trappeform.

- Tjek om systemet er **konsistent**
- Brug **tilbageinds√¶ttelse** for at finde l√∏sninger, hvis systemet har l√∏sninger

---

## 6. Hvad er totalmatricen tilknyttet Ax = b?

Den **totalmatricen** er matrixen dannet ved at s√¶tte h√∏jresiden $b$ sammen med koefficientmatrixen $A$:

$$
[A \mid b]
$$

---

## 7. Hvilke element√¶re r√¶kkeoperationer kender du?

1. Ombytning af to r√¶kker
2. Multiplikation af en r√¶kke med en ikke-nul konstant
3. L√¶gge en multiplum af √©n r√¶kke til en anden

---

## 8. Hvad er en trappeform og hvad er specielt ved en reduceret trappeform?

**Trappeform:**
- Alle nuller√¶kker nederst
- F√∏rste ikke-nul element i en r√¶kke (pivot) er l√¶ngere til h√∏jre end i r√¶kken ovenfor
- Pivotelementet er ofte 1

**Reduceret trappeform:**
- Alle pivotelementer er 1
- Alle andre elementer i pivot-s√∏jlerne er 0

---

## 9. Hvordan kan man afl√¶se fra en trappeform af totalmatricen, om systemet er l√∏seligt eller ej, og hvor mange l√∏sninger man har hvis systemet er l√∏seligt? Hvordan kan man se, om en variable er fri? Hvordan kan man bestemme l√∏sninger ud fra en trappeform, hvis systemet er konsistent?

- Hvis der er en r√¶kke som $0 = d$, hvor $d \neq 0$: **ingen l√∏sning**
- Hvis antallet af pivot-s√∏jler $<$ antal variable: **fri variable**
- Brug **tilbageinds√¶ttelse** til at finde l√∏sninger

---

## 10. Kan du bringe f√∏lgende totalmatrix p√• trappeform?

$$
\left[\begin{array}{ccccc}
-1 & 2 & 3 & 4 & 5 \\
1 & 3 & 1 & 1 & 0 \\
0 & -1 & 2 & 1 & 1
\end{array}\right]
$$

**L√∏sning (Gauss-elimination):**

1. Byt r√¶kke 1 og 2  
2. Brug r√¶kke 1 til at eliminere elementet i r√¶kke 2  
3. Forts√¶t til trappeform

Resultatet (trappeform):

$$
\left[\begin{array}{ccccc}
1 & 3 & 1 & 1 & 0 \\
0 & 5 & 4 & 5 & 5 \\
0 & 0 & \dots & \dots & \dots
\end{array}\right]
$$

---

## 11. Hvad betyder det, at en matrix er p√• reduceret trappeform?

En matrix er i **reduceret trappeform**, hvis:

- Den er i trappeform
- Alle **pivotelementer** (f√∏rste ikke-nul elementer i hver r√¶kke) er 1
- Alle **andre elementer i pivot-s√∏jlen** er 0

Det g√∏r det nemt at afl√¶se l√∏sninger direkte.

---

## 12. Hvordan kan man l√∏se Ax = b vha. reduceret trappeform?

1. S√¶t den **totalmatricen** op: $[A \mid b]$
2. Brug **Gauss-Jordan elimination** for at opn√• **reduceret trappeform**  
3. Afl√¶s l√∏sningen:  
   - Hvis systemet er entydigt l√∏st: √©n l√∏sning  
   - Hvis fri variable: uendelig mange l√∏sninger, skriv som parameterudtryk  

---

## 13. Hvor mange l√∏sninger har systemet Ax = 0?

Systemet $Ax = 0$ har altid **mindst √©n l√∏sning**: den trivielle $x = 0$

Antal l√∏sninger afh√¶nger af **rangen**:
- Hvis rang $A = n$: kun triviel l√∏sning
- Hvis rang $A < n$: uendelig mange l√∏sninger

---

## 14. Hvorn√•r har systemet Ax = 0 kun den trivielle l√∏sning?
Hvis matrix $A \in \mathbb{R}^{m \times n}$ har **fuld s√∏jlerang**, alts√• $\text{rang}(A) = n$, s√• har systemet kun l√∏sningen:

$$
x = 0
$$

---

## 15. Hvordan defineres rangen af en matrix?

**Rangen** af en matrix $A$ er **antallet af line√¶rt uafh√¶ngige r√¶kker (eller s√∏jler)**.

= antallet af **pivot-elementer** i trappeformen

---

## 16. Hvad siger s√¶tningen om rang og l√∏sning af systemet $Ax = b$?

Systemet $Ax = b$ er **l√∏seligt** hvis og kun hvis:

$$
\text{rang}(A) = \text{rang}([A \mid b])
$$

Hvis l√∏sningen findes, og $\text{rang}(A) = n$: entydig l√∏sning  
Hvis $\text{rang}(A) < n$: uendeligt mange l√∏sninger

---

## 17. Giv eksempler p√• vektorer i $\mathbb{R}^n$

Eksempel p√• en vektor i $\mathbb{R}^3$:

$$
v = \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}
$$

Det er blot en kolonne med 3 reelle tal.

---

## 18. Hvordan defineres line√¶r kombination af vektorer?

En **line√¶r kombination** af vektorer $v_1, v_2, \dots, v_k \in \mathbb{R}^n$ er:

$$
a_1 v_1 + a_2 v_2 + \dots + a_k v_k
$$

hvor $a_i \in \mathbb{R}$

---

## 19. Hvorn√•r siges en m√¶ngde af vektorer at v√¶re line√¶rt afh√¶ngige/uafh√¶ngige?

- **Afh√¶ngige:** En vektor kan skrives som line√¶r kombination af de andre  
- **Uafh√¶ngige:** Ingen vektor kan skrives som line√¶r kombination af de andre

Formelt:  
$$
a_1 v_1 + \dots + a_k v_k = 0 \quad \Rightarrow \quad a_1 = \dots = a_k = 0
$$

---
## 20. Hvad er s√∏jlerummet for en matrix?

**S√∏jlerummet** (column space) for matrix $A \in \mathbb{R}^{m \times n}$ er:

$$
\text{Col}(A) = \{ Ax \mid x \in \mathbb{R}^n \} \subseteq \mathbb{R}^m
$$

= m√¶ngden af alle line√¶re kombinationer af s√∏jlerne i $A$

## 21. Hvordan udregner man determinanter af 2√ó2 og 3√ó3-matricer?

- **2√ó2 matrix:**

$$
\det\begin{bmatrix} a & b \\ c & d \end{bmatrix} = ad - bc
$$

- **3√ó3 matrix:**

Brug sarrus-reglen eller kofaktorudvikling:

$$
\det\begin{bmatrix}
a & b & c \\
d & e & f \\
g & h & i
\end{bmatrix}
= aei + bfg + cdh - ceg - bdi - afh
$$

---

## 22. Hvad betyder kofaktorudvidelse?

Kofaktorudvidelse (Laplace-udvikling) betyder at bestemme determinanten af en matrix ved at:

- Udvikle langs en r√¶kke eller s√∏jle:

$$
\det(A) = \sum_{j=1}^{n} (-1)^{i+j} a_{ij} \cdot \det(A_{ij})
$$

hvor $A_{ij}$ er undermatricen uden r√¶kke $i$ og s√∏jle $j$.

---

## 23. Hvad sker med $\det(A)$, n√•r man bytter to r√¶kker eller s√∏jler?

- N√•r man bytter to **r√¶kker** eller **s√∏jler** i $A$, s√• √¶ndrer determinanten **fortegn**:

$$
\det(\text{byttet } A) = -\det(A)
$$

---

## 24. Hvad sker med $\det(A)$, n√•r man l√¶gger et multiplum af √©n r√¶kke til en anden?

- Determinanten **√¶ndrer ikke v√¶rdi**:

$$
\det(A) = \det(A + \lambda \cdot \text{anden r√¶kke})
$$

G√¶lder ogs√• for s√∏jler.

---

## 25. Hvad g√¶lder for determinanter under transponering og multiplikation?

- 
$$
\det(A^T) = \det(A)
$$
- 
$$
\det(AB) = \det(A) \cdot \det(B)
$$

---

## 26. Hvis $A$ er inverterbar, hvad er $\det(A^{-1})$?

$$
\det(A^{-1}) = \frac{1}{\det(A)}
$$

Fordi:

$$
\det(I) = \det(A A^{-1}) = \det(A) \cdot \det(A^{-1}) = 1
$$

---

## 27. Antag at $S$ er symmetrisk og inverterbar. Vis at $S^{-1}$ ogs√• er symmetrisk.

Hvis $S = S^T$, og $S$ er inverterbar:

- Tag transponatet: 

$$
(S^{-1})^T = (S^T)^{-1} = S^{-1}
$$

- Alts√•: $S^{-1} = (S^{-1})^T$, dvs. **symmetrisk**

---

## 28. Hvad er en ortogonal matrix $U$?

En kvadratisk matrix $U$ er **ortogonal**, hvis:

$$
U^{-1} = U^T \quad \text{eller} \quad U^T U = I
$$

Det betyder, at s√∏jlerne i $U$ er **ortonormale**.

---

## 29. Er produktet $U_1 U_2$ af to ortogonale matricer igen ortogonal?

Ja!

- Hvis $U_1$ og $U_2$ er ortogonale:

$$
(U_1 U_2)^T (U_1 U_2) = U_2^T U_1^T U_1 U_2 = U_2^T I U_2 = U_2^T U_2 = I
$$

Alts√• er $U_1 U_2$ ortogonal.

---

## 30. Hvad forst√•r vi ved en linearkombination af vektorerne $v_1, ..., v_k$?

En **linearkombination** af vektorer $v_1, ..., v_k$ er:

$$
a_1 v_1 + a_2 v_2 + \dots + a_k v_k
\quad \text{hvor } a_i \in \mathbb{R}
$$

Det er en m√•de at danne nye vektorer ud fra givne vektorer.

## 31. Hvad er $\text{span}\{v_1, \dots, v_k\}$?

Det er m√¶ngden af alle linearkombinationer af $v_1, \dots, v_k$:

$$
\text{span}\{v_1, \dots, v_k\} = \left\{ \sum_{i=1}^k a_i v_i \mid a_i \in \mathbb{R} \right\}
$$

Span er alts√• et underrum best√•ende af alle vektorer, man kan danne ved linearkombination.

---

## 32. Hvorn√•r siger vi, at vektorerne $v_1, \dots, v_k$ er line√¶rt uafh√¶ngige?

De er line√¶rt uafh√¶ngige, hvis:

$$
c_1 v_1 + \dots + c_k v_k = 0 \Rightarrow c_1 = \dots = c_k = 0
$$

Dvs. den eneste linearkombination, der giver nulvektoren, er den trivielle.

---

## 33. Hvad er et underrum $V \subseteq \mathbb{R}^n$?

Et underrum $V$ er en delm√¶ngde af $\mathbb{R}^n$, som opfylder:

- $0 \in V$
- Lukket under addition: $u, v \in V \Rightarrow u + v \in V$
- Lukket under skalering: $c \in \mathbb{R}, v \in V \Rightarrow c \cdot v \in V$

---

## 34. Hvad er nulrummet til en givet matrix $A$?

Nulrummet er m√¶ngden af l√∏sninger til $Ax = 0$:

$$
\text{Nul}(A) = \{ x \in \mathbb{R}^n \mid Ax = 0 \}
$$

Det er et underrum af $\mathbb{R}^n$.

---

## 35. Hvad er s√∏jlerummet til en givet matrix $A$?

S√∏jlerummet er m√¶ngden af alle linearkombinationer af s√∏jlerne i $A$:

$$
\text{Col}(A) = \text{span}\{\text{s√∏jlerne i } A\}
$$

Det er et underrum af $\mathbb{R}^m$, hvor $A$ er $m \times n$.

---

## 36. Hvad er rang af en matrix?

Rangen af $A$ er dimensionen af s√∏jlerummet:

$$
\text{rang}(A) = \dim(\text{Col}(A))
$$

Det er ogs√• antal ledende 1-taller i en reduceret trappeform.

---

## 37. Hvad er en basis af et underrum $V \subseteq \mathbb{R}^n$?

En basis er en m√¶ngde vektorer:

- Som **sp√¶nder over** $V$
- Som er **line√¶rt uafh√¶ngige**

$$
\text{Basis } \Rightarrow \text{minimalt s√¶t, der genererer } V
$$

---

## 38. Hvad kan man sige om s√∏jlerne i en inverterbar $n \times n$-matrix?

De danner en basis for $\mathbb{R}^n$:

$$
A \text{ inverterbar } \Leftrightarrow \text{s√∏jlerne i } A \text{ er line√¶rt uafh√¶ngige og sp√¶nder over } \mathbb{R}^n
$$

---

## 39. Hvorn√•r er to vektorer $v$ og $w$ ortogonale?

Hvis deres indre produkt er 0:

$$
v \cdot w = 0 \Rightarrow v \perp w
$$

---

## 40. Hvad er formlen for den ortogonale projektion af \(x\) p√• \(\text{span}\{v_1, \dots, v_k\}\), hvis de er ortogonale?

Projektionen \(P x\) er givet ved:

$$
P x = \sum_{j=1}^k \frac{v_j \cdot x}{v_j \cdot v_j} v_j
$$

G√¶lder kun hvis \(v_j\)'erne er parvist ortogonale (ortogonal m√¶ngde).

## 41. Hvad er en ortonormalbasis af \(\mathbb{R}^n\)?

En ortonormalbasis er en basis hvor vektorerne b√•de er

- **Ortogonale:** $v_i \cdot v_j = 0$ for $i \neq j$  
- **Normerede:** $\|v_i\| = 1$

Dvs. vektorerne er ortogonale og har l√¶ngde 1.

---

## 42. Hvad kan man sige om s√∏jlerne i en ortogonal \(n \times n\)-matrix \(U\)?

S√∏jlerne i \(U\) danner en ortonormalbasis for $\mathbb{R}^n$

Det betyder ogs√•:

$$
U^T U = U U^T = I
$$

---

## 43. Hvad er en egenv√¶rdi af en \(n \times n\)-matrix \(A\)?

Et tal \(\omega\), s√•dan at der findes en ikke-nul vektor $v \in \mathbb{R}^n$ med

$$
A v = \omega v
$$

Vektoren $v$ kaldes en egenvektor til egenv√¶rdien $\omega$.

---

## 44. Hvordan kan jeg bestemme egenv√¶rdierne af \(A\)?

Egenv√¶rdierne er nulpunkterne til det karakteristiske polynomium:

$$
\det(A - \omega I) = 0
$$

L√∏sning af denne ligning giver egenv√¶rdierne $\omega$

---

## 45. Hvad er egenrummet af $A$ til egenv√¶rdien $\omega$?

Egenrummet er m√¶ngden af alle egenvektorer til $\omega$ plus nulvektoren:

$$
\text{Nul}(A - \omega I) = \{ v \mid (A - \omega I)v = 0 \}
$$

---

## 46. Hvordan kan jeg bestemme egenvektorer $v$ til egenv√¶rdien $\omega$?

L√∏s det homogene ligningssystem:

$$
(A - \omega I) v = 0
$$

Alle l√∏sninger $v \neq 0$ er egenvektorer til $\omega$.

---

## 47. Hvorn√•r kaldes en $n \times n -matrix  A$ diagonaliserbar?

Hvis den er simil√¶r med en diagonal matrix $D$, dvs. hvis der findes en inverterbar matrix $P$, s√•

$$
A = P D P^{-1}
$$

---

## 48. Hvis $A = P D P^{-1}$ er en diagonalisering af $A$, hvad er indgangene i $D$ og $P$?

- Diagonalen i $D$ indeholder egenv√¶rdierne til $A$.
- S√∏jlerne i $P$ er egenvektorer til $A$, i samme r√¶kkef√∏lge som egenv√¶rdierne i $D$.

---

## 49. Er alle kvadratiske matricer diagonaliserbare?

Nej, ikke alle er diagonaliserbare.

---

## 50. Hvorn√•r er en kvadratisk matrix \(A\) diagonaliserbar?

Hvis og kun hvis for hver egenv√¶rdi stemmer den algebraiske multiplicitet overens med den geometriske multiplicitet.

## 51. Hvad ved man om symmetriske matricer?

En symmetrisk matrix $S$ er altid diagonaliserbar ‚Äî faktisk er $S$ endda ortogonalt diagonaliserbar, dvs. der findes en diagonal matrix $D$ og en ortogonal matrix $U$ s√•dan at

$$
S = U D U^T
$$

(hvor $U^T = U^{-1}$, fordi $U$ er ortogonal).

---

## 52. Hvad er en mindste kvadraters l√∏sning til $Ax = b$?

En vektor $\hat{x}$, s√•dan at

$$
|b - A \hat{x}| \leq \|b - A x\| \quad \text{for alle } x
$$

Dvs. $\hat{x}$ minimerer den kvadrerede afstand mellem $b$ og $A x$.

---

## 53. Hvordan kan man bestemme mindste kvadraters l√∏sninger?

$\hat{x}$ er mindste kvadraters l√∏sning til $Ax = b$ hvis og kun hvis $\hat{x}$ l√∏ser normalligningen:

$$
A^T A \hat{x} = A^T b
$$

---

## 54. Er mindste kvadraters l√∏sninger entydige?

Ikke altid. De er entydige hvis og kun hvis $A$ har fuld rang, eller √¶kvivalent hvis $A^T A$ er inverterbar.

---

## 55. Hvorn√•r giver det mening at bruge mindste kvadraters metoden?

N√•r man ikke har nogen grund til at tro, eller ikke er sikker p√•, at det oprindelige system $Ax = b$ er konsistent.

---

## 56. Hvad, hvis jeg udregner en mindste kvadraters l√∏sning $\hat{x}$ til et system $Ax = b$, som faktisk er konsistent?

S√• er enhver mindste kvadraters l√∏sning $\hat{x}$ ogs√• en almindelig l√∏sning til $Ax = b$, dvs.

$$
A \hat{x} = b
$$

---

## 57. Hvad er et line√¶rt programmeringsproblem p√• kanonisk form?

Maksimer

$$
c^T x
$$

under bibetingelserne

$$
A x \leq b, \quad x \geq 0
$$

---

## 58. Hvordan kan bibetingelser som $a_i^T x \geq b_i$ eller $a_i^T x = b_i$ omskrives i et line√¶rt programmeringsproblem p√• kanonisk form?

- $a_i^T x \geq b_i$ omskrives som $-a_i^T x \leq -b_i$
- $a_i^T x = b_i$ omskrives som to uligheder: $a_i^T x \leq b_i$ og $a_i^T x \geq b_i$

---

## 59. Hvad, hvis man vil minimere $c^T x$: kan man stadigv√¶k opstille et line√¶rt programmeringsproblem p√• kanonisk form?

Ja, minimere $c^T x$ kan omskrives til maksimering af $-c^T x$.

---

## 60. Kender du to situationer, hvor man ikke finder en optimal l√∏sning til et line√¶rt programmeringsproblem?

1. Feasibility set $F$ er tomt (ingen l√∏sninger).
2. $c^T x$ er ubegr√¶nset p√• $F$ (ingen maksimum).

---

## 61. Hvis $F$ ikke er tom og $c^T x$ er begr√¶nset p√• $F$, hvor i $F$ ligger de optimale l√∏sninger?

Mindst √©n optimal l√∏sning findes i et ekstremalpunkt (hj√∏rnepunkt) af den konvekse m√¶ngde $F$

---

## 62. Skitser m√¶ngden $F$ af alle vektorer $x = (x_1, x_2)$, der opfylder

$$
0 \leq x_1 \leq 2, \quad 0 \leq x_2 \leq 2, \quad x_1 + x_2 \leq 3
$$

Hvilke punkter i $F$ ville du afpr√∏ve som mulige optimale l√∏sninger i den grafiske metode?

Svar: De mulige optimale l√∏sninger ligger i hj√∏rnerne af $F$:

$$
(0,0), (2,0), (0,2), (2,1), (1,2)
$$


---

## 63. Hvordan opstiller man en initial simplextableau (begyndelsessimplextableau) for et line√¶rt programmeringsproblem p√• kanonisk form?

- Tilf√∏j slack-variabler for at omskrive uligheder til ligheder.
- Antal slack-variabler svarer til antallet af uligheder.
- Objektfunktionen tilf√∏jes som sidste r√¶kke i tableau.

---

## 64. Beskriv algoritmen i simplexmetoden i tilf√¶lde, hvor alle indgange i $b$ er positive!

- V√¶lg indgangsvariabel (variabel med mest negativ koefficient i objektfunktionen).
- Bestem udgangsvariabel via minimum ratio test.
- Udf√∏r pivotering.
- Gentag indtil ingen negative koefficienter i objektfunktionen.
- Optimal l√∏sning afl√¶ses i tableau.

---

## 65. Kan man g√∏re noget, hvis $b$ har en eller flere negative indgange?

Ja, man kan bruge en ‚Äúbig M‚Äù metode eller to-fase simplex til at starte med en basisl√∏sning.

---

## 66. Hvad er det duale problem til et line√¶rt programmeringsproblem p√• kanonisk form?

Hvis primalproblemet er

$$
\max c^T x \quad \text{under} \quad A x \leq b, \quad x \geq 0
$$

s√• er det duale problem

$$
\min b^T y \quad \text{under} \quad A^T y \geq c, \quad y \geq 0
$$

---

## 67. Hvad er udsagnet af dualitetss√¶tningen om line√¶re programmeringsproblemer samt deres duale problemer?

Hvis primalproblemet har en optimal l√∏sning, s√• har dualproblemet ogs√• en optimal l√∏sning, og deres optimale v√¶rdier er ens:

$$
\max c^T x = \min b^T y
$$

Desuden, hvis enten primal eller dual har ubegr√¶nset l√∏sning, s√• er det modsatte problem ufeasible.
