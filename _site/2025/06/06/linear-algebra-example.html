
<hr />
<p>layout: post
title: ‚ÄúLinear Algebra Example‚Äù
date: 2025-06-10
categories: [blog]
‚Äî</p>

<h1 id="-line√¶r-algebra--eksamen-2025">üìò Line√¶r Algebra ‚Äì Eksamen 2025</h1>

<h2 id="1-hvordan-multiplicerer-man-to-matricer-hvilke-st√∏rrelser-skal-de-to-matricer-a-og-b-have-s√•-at-deres-produkt-ab-er-defineret-lille-eksempel-p√•-det-fx-32-matrix-gange-to-dimensionelt-vektor">1. Hvordan multiplicerer man to matricer? Hvilke st√∏rrelser skal de to matricer A og B have, s√• at deres produkt AB er defineret? Lille eksempel p√• det, fx 3√ó2-matrix gange to-dimensionelt vektor.</h2>

<p>To matricer</p>

\[A \in \mathbb{R}^{m \times n} \quad \text{og} \quad B \in \mathbb{R}^{n \times p}\]

<p>kan multipliceres, hvis <strong>antallet af s√∏jler i $A$</strong> er lig med <strong>antallet af r√¶kker i $B$</strong>. Produktet</p>

\[AB \in \mathbb{R}^{m \times p}\]

<p>bliver da en matrix i $\mathbb{R}^{m \times p}$.</p>

<p><strong>Eksempel:</strong></p>

\[A = \begin{bmatrix} 1 &amp; 2 \\ 3 &amp; 4 \\ 5 &amp; 6 \end{bmatrix}, \quad
x = \begin{bmatrix} 7 \\ 8 \end{bmatrix}
\Rightarrow
Ax = \begin{bmatrix} 1\cdot7 + 2\cdot8 \\ 3\cdot7 + 4\cdot8 \\ 5\cdot7 + 6\cdot8 \end{bmatrix} = \begin{bmatrix} 23 \\ 53 \\ 83 \end{bmatrix}\]

<hr />

<h2 id="2-g√¶lder-altid-ab--ba-hvis-a-og-b-begge-er-nn-matricer-nej-oftest-ikke">2. G√¶lder altid AB = BA, hvis A og B begge er n√ón-matricer? (Nej, oftest ikke!)</h2>

<p>Nej, matrixmultiplikation er <strong>ikke kommutativ</strong>. Dvs. generelt g√¶lder:</p>

\[AB \neq BA\]

<p><strong>Undtagelser:</strong> Det kan dog g√¶lde, hvis:</p>
<ul>
  <li>( A = I ) (identitetsmatrix)</li>
  <li>( A ) og ( B ) er diagonale og kommuterer</li>
  <li>( A ) og ( B ) er potenser af samme matrix</li>
</ul>

<hr />

<h2 id="3-hvad-sker-n√•r-man-ganger-matricen-a-fra-venstre-eller-h√∏jre-med-identitetsmatricen-i-af-den-tilpasse-st√∏rrelse-ingenting-man-f√•r-bare-igen-a">3. Hvad sker, n√•r man ganger matricen A fra venstre eller h√∏jre med identitetsmatricen I af den tilpasse st√∏rrelse? (Ingenting, man f√•r bare igen A.)</h2>

<p>Hvis 
\(A = \mathbb{R}^{m \times n} \quad \text{og} \quad I\)
har passende st√∏rrelse, s√• g√¶lder:
\(IA = A \quad \text{og} \quad AI = A\)</p>

<hr />

<h2 id="4-hvor-mange-l√∏sninger-kan-et-line√¶rt-ligningssystem-ax--b-i-princippet-have">4. Hvor mange l√∏sninger kan et line√¶rt ligningssystem Ax = b i princippet have?</h2>

<p>Et line√¶rt system kan have:</p>
<ul>
  <li><strong>Ingen l√∏sning</strong> (inkonsistent)</li>
  <li><strong>√ân entydig l√∏sning</strong></li>
  <li><strong>Uendeligt mange l√∏sninger</strong></li>
</ul>

<hr />

<h2 id="5-hvordan-kan-man-unders√∏ge-om-ax--b-har-l√∏sninger-og-i-givet-fald-finde-dem">5. Hvordan kan man unders√∏ge, om Ax = b har l√∏sninger og i givet fald finde dem?</h2>

<p>Brug <strong>Gauss-elimination</strong> til at omskrive systemet til trappeform.</p>

<ul>
  <li>Tjek om systemet er <strong>konsistent</strong></li>
  <li>Brug <strong>tilbageinds√¶ttelse</strong> for at finde l√∏sninger, hvis systemet har l√∏sninger</li>
</ul>

<hr />

<h2 id="6-hvad-er-totalmatricen-tilknyttet-ax--b">6. Hvad er totalmatricen tilknyttet Ax = b?</h2>

<p>Den <strong>totalmatricen</strong> er matrixen dannet ved at s√¶tte h√∏jresiden $b$ sammen med koefficientmatrixen $A$:</p>

\[[A \mid b]\]

<hr />

<h2 id="7-hvilke-element√¶re-r√¶kkeoperationer-kender-du">7. Hvilke element√¶re r√¶kkeoperationer kender du?</h2>

<ol>
  <li>Ombytning af to r√¶kker</li>
  <li>Multiplikation af en r√¶kke med en ikke-nul konstant</li>
  <li>L√¶gge en multiplum af √©n r√¶kke til en anden</li>
</ol>

<hr />

<h2 id="8-hvad-er-en-trappeform-og-hvad-er-specielt-ved-en-reduceret-trappeform">8. Hvad er en trappeform og hvad er specielt ved en reduceret trappeform?</h2>

<p><strong>Trappeform:</strong></p>
<ul>
  <li>Alle nuller√¶kker nederst</li>
  <li>F√∏rste ikke-nul element i en r√¶kke (pivot) er l√¶ngere til h√∏jre end i r√¶kken ovenfor</li>
  <li>Pivotelementet er ofte 1</li>
</ul>

<p><strong>Reduceret trappeform:</strong></p>
<ul>
  <li>Alle pivotelementer er 1</li>
  <li>Alle andre elementer i pivot-s√∏jlerne er 0</li>
</ul>

<hr />

<h2 id="9-hvordan-kan-man-afl√¶se-fra-en-trappeform-af-totalmatricen-om-systemet-er-l√∏seligt-eller-ej-og-hvor-mange-l√∏sninger-man-har-hvis-systemet-er-l√∏seligt-hvordan-kan-man-se-om-en-variable-er-fri-hvordan-kan-man-bestemme-l√∏sninger-ud-fra-en-trappeform-hvis-systemet-er-konsistent">9. Hvordan kan man afl√¶se fra en trappeform af totalmatricen, om systemet er l√∏seligt eller ej, og hvor mange l√∏sninger man har hvis systemet er l√∏seligt? Hvordan kan man se, om en variable er fri? Hvordan kan man bestemme l√∏sninger ud fra en trappeform, hvis systemet er konsistent?</h2>

<ul>
  <li>Hvis der er en r√¶kke som $0 = d$, hvor $d \neq 0$: <strong>ingen l√∏sning</strong></li>
  <li>Hvis antallet af pivot-s√∏jler $&lt;$ antal variable: <strong>fri variable</strong></li>
  <li>Brug <strong>tilbageinds√¶ttelse</strong> til at finde l√∏sninger</li>
</ul>

<hr />

<h2 id="10-kan-du-bringe-f√∏lgende-totalmatrix-p√•-trappeform">10. Kan du bringe f√∏lgende totalmatrix p√• trappeform?</h2>

\[\left[\begin{array}{ccccc}
-1 &amp; 2 &amp; 3 &amp; 4 &amp; 5 \\
1 &amp; 3 &amp; 1 &amp; 1 &amp; 0 \\
0 &amp; -1 &amp; 2 &amp; 1 &amp; 1
\end{array}\right]\]

<p><strong>L√∏sning (Gauss-elimination):</strong></p>

<ol>
  <li>Byt r√¶kke 1 og 2</li>
  <li>Brug r√¶kke 1 til at eliminere elementet i r√¶kke 2</li>
  <li>Forts√¶t til trappeform</li>
</ol>

<p>Resultatet (trappeform):</p>

\[\left[\begin{array}{ccccc}
1 &amp; 3 &amp; 1 &amp; 1 &amp; 0 \\
0 &amp; 5 &amp; 4 &amp; 5 &amp; 5 \\
0 &amp; 0 &amp; \dots &amp; \dots &amp; \dots
\end{array}\right]\]

<hr />

<h2 id="11-hvad-betyder-det-at-en-matrix-er-p√•-reduceret-trappeform">11. Hvad betyder det, at en matrix er p√• reduceret trappeform?</h2>

<p>En matrix er i <strong>reduceret trappeform</strong>, hvis:</p>

<ul>
  <li>Den er i trappeform</li>
  <li>Alle <strong>pivotelementer</strong> (f√∏rste ikke-nul elementer i hver r√¶kke) er 1</li>
  <li>Alle <strong>andre elementer i pivot-s√∏jlen</strong> er 0</li>
</ul>

<p>Det g√∏r det nemt at afl√¶se l√∏sninger direkte.</p>

<hr />

<h2 id="12-hvordan-kan-man-l√∏se-ax--b-vha-reduceret-trappeform">12. Hvordan kan man l√∏se Ax = b vha. reduceret trappeform?</h2>

<ol>
  <li>S√¶t den <strong>totalmatricen</strong> op: $[A \mid b]$</li>
  <li>Brug <strong>Gauss-Jordan elimination</strong> for at opn√• <strong>reduceret trappeform</strong></li>
  <li>Afl√¶s l√∏sningen:
    <ul>
      <li>Hvis systemet er entydigt l√∏st: √©n l√∏sning</li>
      <li>Hvis fri variable: uendelig mange l√∏sninger, skriv som parameterudtryk</li>
    </ul>
  </li>
</ol>

<hr />

<h2 id="13-hvor-mange-l√∏sninger-har-systemet-ax--0">13. Hvor mange l√∏sninger har systemet Ax = 0?</h2>

<p>Systemet $Ax = 0$ har altid <strong>mindst √©n l√∏sning</strong>: den trivielle $x = 0$</p>

<p>Antal l√∏sninger afh√¶nger af <strong>rangen</strong>:</p>
<ul>
  <li>Hvis rang $A = n$: kun triviel l√∏sning</li>
  <li>Hvis rang $A &lt; n$: uendelig mange l√∏sninger</li>
</ul>

<hr />

<h2 id="14-hvorn√•r-har-systemet-ax--0-kun-den-trivielle-l√∏sning">14. Hvorn√•r har systemet Ax = 0 kun den trivielle l√∏sning?</h2>
<p>Hvis matrix $A \in \mathbb{R}^{m \times n}$ har <strong>fuld s√∏jlerang</strong>, alts√• $\text{rang}(A) = n$, s√• har systemet kun l√∏sningen:</p>

\[x = 0\]

<hr />

<h2 id="15-hvordan-defineres-rangen-af-en-matrix">15. Hvordan defineres rangen af en matrix?</h2>

<p><strong>Rangen</strong> af en matrix $A$ er <strong>antallet af line√¶rt uafh√¶ngige r√¶kker (eller s√∏jler)</strong>.</p>

<p>= antallet af <strong>pivot-elementer</strong> i trappeformen</p>

<hr />

<h2 id="16-hvad-siger-s√¶tningen-om-rang-og-l√∏sning-af-systemet-ax--b">16. Hvad siger s√¶tningen om rang og l√∏sning af systemet $Ax = b$?</h2>

<p>Systemet $Ax = b$ er <strong>l√∏seligt</strong> hvis og kun hvis:</p>

\[\text{rang}(A) = \text{rang}([A \mid b])\]

<p>Hvis l√∏sningen findes, og $\text{rang}(A) = n$: entydig l√∏sning<br />
Hvis $\text{rang}(A) &lt; n$: uendeligt mange l√∏sninger</p>

<hr />

<h2 id="17-giv-eksempler-p√•-vektorer-i-mathbbrn">17. Giv eksempler p√• vektorer i $\mathbb{R}^n$</h2>

<p>Eksempel p√• en vektor i $\mathbb{R}^3$:</p>

\[v = \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}\]

<p>Det er blot en kolonne med 3 reelle tal.</p>

<hr />

<h2 id="18-hvordan-defineres-line√¶r-kombination-af-vektorer">18. Hvordan defineres line√¶r kombination af vektorer?</h2>

<p>En <strong>line√¶r kombination</strong> af vektorer $v_1, v_2, \dots, v_k \in \mathbb{R}^n$ er:</p>

\[a_1 v_1 + a_2 v_2 + \dots + a_k v_k\]

<p>hvor $a_i \in \mathbb{R}$</p>

<hr />

<h2 id="19-hvorn√•r-siges-en-m√¶ngde-af-vektorer-at-v√¶re-line√¶rt-afh√¶ngigeuafh√¶ngige">19. Hvorn√•r siges en m√¶ngde af vektorer at v√¶re line√¶rt afh√¶ngige/uafh√¶ngige?</h2>

<ul>
  <li><strong>Afh√¶ngige:</strong> En vektor kan skrives som line√¶r kombination af de andre</li>
  <li><strong>Uafh√¶ngige:</strong> Ingen vektor kan skrives som line√¶r kombination af de andre</li>
</ul>

<p>Formelt:<br />
\(a_1 v_1 + \dots + a_k v_k = 0 \quad \Rightarrow \quad a_1 = \dots = a_k = 0\)</p>

<hr />
<h2 id="20-hvad-er-s√∏jlerummet-for-en-matrix">20. Hvad er s√∏jlerummet for en matrix?</h2>

<p><strong>S√∏jlerummet</strong> (column space) for matrix $A \in \mathbb{R}^{m \times n}$ er:</p>

\[\text{Col}(A) = \{ Ax \mid x \in \mathbb{R}^n \} \subseteq \mathbb{R}^m\]

<p>= m√¶ngden af alle line√¶re kombinationer af s√∏jlerne i $A$</p>

<h2 id="21-hvordan-udregner-man-determinanter-af-22-og-33-matricer">21. Hvordan udregner man determinanter af 2√ó2 og 3√ó3-matricer?</h2>

<ul>
  <li><strong>2√ó2 matrix:</strong></li>
</ul>

\[\det\begin{bmatrix} a &amp; b \\ c &amp; d \end{bmatrix} = ad - bc\]

<ul>
  <li><strong>3√ó3 matrix:</strong></li>
</ul>

<p>Brug sarrus-reglen eller kofaktorudvikling:</p>

\[\det\begin{bmatrix}
a &amp; b &amp; c \\
d &amp; e &amp; f \\
g &amp; h &amp; i
\end{bmatrix}
= aei + bfg + cdh - ceg - bdi - afh\]

<hr />

<h2 id="22-hvad-betyder-kofaktorudvidelse">22. Hvad betyder kofaktorudvidelse?</h2>

<p>Kofaktorudvidelse (Laplace-udvikling) betyder at bestemme determinanten af en matrix ved at:</p>

<ul>
  <li>Udvikle langs en r√¶kke eller s√∏jle:</li>
</ul>

\[\det(A) = \sum_{j=1}^{n} (-1)^{i+j} a_{ij} \cdot \det(A_{ij})\]

<p>hvor $A_{ij}$ er undermatricen uden r√¶kke $i$ og s√∏jle $j$.</p>

<hr />

<h2 id="23-hvad-sker-med-deta-n√•r-man-bytter-to-r√¶kker-eller-s√∏jler">23. Hvad sker med $\det(A)$, n√•r man bytter to r√¶kker eller s√∏jler?</h2>

<ul>
  <li>N√•r man bytter to <strong>r√¶kker</strong> eller <strong>s√∏jler</strong> i $A$, s√• √¶ndrer determinanten <strong>fortegn</strong>:</li>
</ul>

\[\det(\text{byttet } A) = -\det(A)\]

<hr />

<h2 id="24-hvad-sker-med-deta-n√•r-man-l√¶gger-et-multiplum-af-√©n-r√¶kke-til-en-anden">24. Hvad sker med $\det(A)$, n√•r man l√¶gger et multiplum af √©n r√¶kke til en anden?</h2>

<ul>
  <li>Determinanten <strong>√¶ndrer ikke v√¶rdi</strong>:</li>
</ul>

\[\det(A) = \det(A + \lambda \cdot \text{anden r√¶kke})\]

<p>G√¶lder ogs√• for s√∏jler.</p>

<hr />

<h2 id="25-hvad-g√¶lder-for-determinanter-under-transponering-og-multiplikation">25. Hvad g√¶lder for determinanter under transponering og multiplikation?</h2>

<ul>
  <li>
\[\det(A^T) = \det(A)\]
  </li>
  <li>
\[\det(AB) = \det(A) \cdot \det(B)\]
  </li>
</ul>

<hr />

<h2 id="26-hvis-a-er-inverterbar-hvad-er-deta-1">26. Hvis $A$ er inverterbar, hvad er $\det(A^{-1})$?</h2>

\[\det(A^{-1}) = \frac{1}{\det(A)}\]

<p>Fordi:</p>

\[\det(I) = \det(A A^{-1}) = \det(A) \cdot \det(A^{-1}) = 1\]

<hr />

<h2 id="27-antag-at-s-er-symmetrisk-og-inverterbar-vis-at-s-1-ogs√•-er-symmetrisk">27. Antag at $S$ er symmetrisk og inverterbar. Vis at $S^{-1}$ ogs√• er symmetrisk.</h2>

<p>Hvis $S = S^T$, og $S$ er inverterbar:</p>

<ul>
  <li>Tag transponatet:</li>
</ul>

\[(S^{-1})^T = (S^T)^{-1} = S^{-1}\]

<ul>
  <li>Alts√•: $S^{-1} = (S^{-1})^T$, dvs. <strong>symmetrisk</strong></li>
</ul>

<hr />

<h2 id="28-hvad-er-en-ortogonal-matrix-u">28. Hvad er en ortogonal matrix $U$?</h2>

<p>En kvadratisk matrix $U$ er <strong>ortogonal</strong>, hvis:</p>

\[U^{-1} = U^T \quad \text{eller} \quad U^T U = I\]

<p>Det betyder, at s√∏jlerne i $U$ er <strong>ortonormale</strong>.</p>

<hr />

<h2 id="29-er-produktet-u_1-u_2-af-to-ortogonale-matricer-igen-ortogonal">29. Er produktet $U_1 U_2$ af to ortogonale matricer igen ortogonal?</h2>

<p>Ja!</p>

<ul>
  <li>Hvis $U_1$ og $U_2$ er ortogonale:</li>
</ul>

\[(U_1 U_2)^T (U_1 U_2) = U_2^T U_1^T U_1 U_2 = U_2^T I U_2 = U_2^T U_2 = I\]

<p>Alts√• er $U_1 U_2$ ortogonal.</p>

<hr />

<h2 id="30-hvad-forst√•r-vi-ved-en-linearkombination-af-vektorerne-v_1--v_k">30. Hvad forst√•r vi ved en linearkombination af vektorerne $v_1, ‚Ä¶, v_k$?</h2>

<p>En <strong>linearkombination</strong> af vektorer $v_1, ‚Ä¶, v_k$ er:</p>

\[a_1 v_1 + a_2 v_2 + \dots + a_k v_k
\quad \text{hvor } a_i \in \mathbb{R}\]

<p>Det er en m√•de at danne nye vektorer ud fra givne vektorer.</p>

<h2 id="31-hvad-er-textspanv_1-dots-v_k">31. Hvad er $\text{span}{v_1, \dots, v_k}$?</h2>

<p>Det er m√¶ngden af alle linearkombinationer af $v_1, \dots, v_k$:</p>

\[\text{span}\{v_1, \dots, v_k\} = \left\{ \sum_{i=1}^k a_i v_i \mid a_i \in \mathbb{R} \right\}\]

<p>Span er alts√• et underrum best√•ende af alle vektorer, man kan danne ved linearkombination.</p>

<hr />

<h2 id="32-hvorn√•r-siger-vi-at-vektorerne-v_1-dots-v_k-er-line√¶rt-uafh√¶ngige">32. Hvorn√•r siger vi, at vektorerne $v_1, \dots, v_k$ er line√¶rt uafh√¶ngige?</h2>

<p>De er line√¶rt uafh√¶ngige, hvis:</p>

\[c_1 v_1 + \dots + c_k v_k = 0 \Rightarrow c_1 = \dots = c_k = 0\]

<p>Dvs. den eneste linearkombination, der giver nulvektoren, er den trivielle.</p>

<hr />

<h2 id="33-hvad-er-et-underrum-v-subseteq-mathbbrn">33. Hvad er et underrum $V \subseteq \mathbb{R}^n$?</h2>

<p>Et underrum $V$ er en delm√¶ngde af $\mathbb{R}^n$, som opfylder:</p>

<ul>
  <li>$0 \in V$</li>
  <li>Lukket under addition: $u, v \in V \Rightarrow u + v \in V$</li>
  <li>Lukket under skalering: $c \in \mathbb{R}, v \in V \Rightarrow c \cdot v \in V$</li>
</ul>

<hr />

<h2 id="34-hvad-er-nulrummet-til-en-givet-matrix-a">34. Hvad er nulrummet til en givet matrix $A$?</h2>

<p>Nulrummet er m√¶ngden af l√∏sninger til $Ax = 0$:</p>

\[\text{Nul}(A) = \{ x \in \mathbb{R}^n \mid Ax = 0 \}\]

<p>Det er et underrum af $\mathbb{R}^n$.</p>

<hr />

<h2 id="35-hvad-er-s√∏jlerummet-til-en-givet-matrix-a">35. Hvad er s√∏jlerummet til en givet matrix $A$?</h2>

<p>S√∏jlerummet er m√¶ngden af alle linearkombinationer af s√∏jlerne i $A$:</p>

\[\text{Col}(A) = \text{span}\{\text{s√∏jlerne i } A\}\]

<p>Det er et underrum af $\mathbb{R}^m$, hvor $A$ er $m \times n$.</p>

<hr />

<h2 id="36-hvad-er-rang-af-en-matrix">36. Hvad er rang af en matrix?</h2>

<p>Rangen af $A$ er dimensionen af s√∏jlerummet:</p>

\[\text{rang}(A) = \dim(\text{Col}(A))\]

<p>Det er ogs√• antal ledende 1-taller i en reduceret trappeform.</p>

<hr />

<h2 id="37-hvad-er-en-basis-af-et-underrum-v-subseteq-mathbbrn">37. Hvad er en basis af et underrum $V \subseteq \mathbb{R}^n$?</h2>

<p>En basis er en m√¶ngde vektorer:</p>

<ul>
  <li>Som <strong>sp√¶nder over</strong> $V$</li>
  <li>Som er <strong>line√¶rt uafh√¶ngige</strong></li>
</ul>

\[\text{Basis } \Rightarrow \text{minimalt s√¶t, der genererer } V\]

<hr />

<h2 id="38-hvad-kan-man-sige-om-s√∏jlerne-i-en-inverterbar-n-times-n-matrix">38. Hvad kan man sige om s√∏jlerne i en inverterbar $n \times n$-matrix?</h2>

<p>De danner en basis for $\mathbb{R}^n$:</p>

\[A \text{ inverterbar } \Leftrightarrow \text{s√∏jlerne i } A \text{ er line√¶rt uafh√¶ngige og sp√¶nder over } \mathbb{R}^n\]

<hr />

<h2 id="39-hvorn√•r-er-to-vektorer-v-og-w-ortogonale">39. Hvorn√•r er to vektorer $v$ og $w$ ortogonale?</h2>

<p>Hvis deres indre produkt er 0:</p>

\[v \cdot w = 0 \Rightarrow v \perp w\]

<hr />

<h2 id="40-hvad-er-formlen-for-den-ortogonale-projektion-af-x-p√•-textspanv_1-dots-v_k-hvis-de-er-ortogonale">40. Hvad er formlen for den ortogonale projektion af (x) p√• (\text{span}{v_1, \dots, v_k}), hvis de er ortogonale?</h2>

<p>Projektionen (P x) er givet ved:</p>

\[P x = \sum_{j=1}^k \frac{v_j \cdot x}{v_j \cdot v_j} v_j\]

<p>G√¶lder kun hvis (v_j)‚Äòerne er parvist ortogonale (ortogonal m√¶ngde).</p>

<h2 id="41-hvad-er-en-ortonormalbasis-af-mathbbrn">41. Hvad er en ortonormalbasis af (\mathbb{R}^n)?</h2>

<p>En ortonormalbasis er en basis hvor vektorerne b√•de er</p>

<ul>
  <li><strong>Ortogonale:</strong> $v_i \cdot v_j = 0$ for $i \neq j$</li>
  <li><strong>Normerede:</strong> $|v_i| = 1$</li>
</ul>

<p>Dvs. vektorerne er ortogonale og har l√¶ngde 1.</p>

<hr />

<h2 id="42-hvad-kan-man-sige-om-s√∏jlerne-i-en-ortogonal-n-times-n-matrix-u">42. Hvad kan man sige om s√∏jlerne i en ortogonal (n \times n)-matrix (U)?</h2>

<p>S√∏jlerne i (U) danner en ortonormalbasis for $\mathbb{R}^n$</p>

<p>Det betyder ogs√•:</p>

\[U^T U = U U^T = I\]

<hr />

<h2 id="43-hvad-er-en-egenv√¶rdi-af-en-n-times-n-matrix-a">43. Hvad er en egenv√¶rdi af en (n \times n)-matrix (A)?</h2>

<p>Et tal (\omega), s√•dan at der findes en ikke-nul vektor $v \in \mathbb{R}^n$ med</p>

\[A v = \omega v\]

<p>Vektoren $v$ kaldes en egenvektor til egenv√¶rdien $\omega$.</p>

<hr />

<h2 id="44-hvordan-kan-jeg-bestemme-egenv√¶rdierne-af-a">44. Hvordan kan jeg bestemme egenv√¶rdierne af (A)?</h2>

<p>Egenv√¶rdierne er nulpunkterne til det karakteristiske polynomium:</p>

\[\det(A - \omega I) = 0\]

<p>L√∏sning af denne ligning giver egenv√¶rdierne $\omega$</p>

<hr />

<h2 id="45-hvad-er-egenrummet-af-a-til-egenv√¶rdien-omega">45. Hvad er egenrummet af $A$ til egenv√¶rdien $\omega$?</h2>

<p>Egenrummet er m√¶ngden af alle egenvektorer til $\omega$ plus nulvektoren:</p>

\[\text{Nul}(A - \omega I) = \{ v \mid (A - \omega I)v = 0 \}\]

<hr />

<h2 id="46-hvordan-kan-jeg-bestemme-egenvektorer-v-til-egenv√¶rdien-omega">46. Hvordan kan jeg bestemme egenvektorer $v$ til egenv√¶rdien $\omega$?</h2>

<p>L√∏s det homogene ligningssystem:</p>

\[(A - \omega I) v = 0\]

<p>Alle l√∏sninger $v \neq 0$ er egenvektorer til $\omega$.</p>

<hr />

<h2 id="47-hvorn√•r-kaldes-en-n-times-n--matrix--a-diagonaliserbar">47. Hvorn√•r kaldes en $n \times n -matrix  A$ diagonaliserbar?</h2>

<p>Hvis den er simil√¶r med en diagonal matrix $D$, dvs. hvis der findes en inverterbar matrix $P$, s√•</p>

\[A = P D P^{-1}\]

<hr />

<h2 id="48-hvis-a--p-d-p-1-er-en-diagonalisering-af-a-hvad-er-indgangene-i-d-og-p">48. Hvis $A = P D P^{-1}$ er en diagonalisering af $A$, hvad er indgangene i $D$ og $P$?</h2>

<ul>
  <li>Diagonalen i $D$ indeholder egenv√¶rdierne til $A$.</li>
  <li>S√∏jlerne i $P$ er egenvektorer til $A$, i samme r√¶kkef√∏lge som egenv√¶rdierne i $D$.</li>
</ul>

<hr />

<h2 id="49-er-alle-kvadratiske-matricer-diagonaliserbare">49. Er alle kvadratiske matricer diagonaliserbare?</h2>

<p>Nej, ikke alle er diagonaliserbare.</p>

<hr />

<h2 id="50-hvorn√•r-er-en-kvadratisk-matrix-a-diagonaliserbar">50. Hvorn√•r er en kvadratisk matrix (A) diagonaliserbar?</h2>

<p>Hvis og kun hvis for hver egenv√¶rdi stemmer den algebraiske multiplicitet overens med den geometriske multiplicitet.</p>

<h2 id="51-hvad-ved-man-om-symmetriske-matricer">51. Hvad ved man om symmetriske matricer?</h2>

<p>En symmetrisk matrix $S$ er altid diagonaliserbar ‚Äî faktisk er $S$ endda ortogonalt diagonaliserbar, dvs. der findes en diagonal matrix $D$ og en ortogonal matrix $U$ s√•dan at</p>

\[S = U D U^T\]

<p>(hvor $U^T = U^{-1}$, fordi $U$ er ortogonal).</p>

<hr />

<h2 id="52-hvad-er-en-mindste-kvadraters-l√∏sning-til-ax--b">52. Hvad er en mindste kvadraters l√∏sning til $Ax = b$?</h2>

<p>En vektor $\hat{x}$, s√•dan at</p>

\[|b - A \hat{x}| \leq \|b - A x\| \quad \text{for alle } x\]

<p>Dvs. $\hat{x}$ minimerer den kvadrerede afstand mellem $b$ og $A x$.</p>

<hr />

<h2 id="53-hvordan-kan-man-bestemme-mindste-kvadraters-l√∏sninger">53. Hvordan kan man bestemme mindste kvadraters l√∏sninger?</h2>

<p>$\hat{x}$ er mindste kvadraters l√∏sning til $Ax = b$ hvis og kun hvis $\hat{x}$ l√∏ser normalligningen:</p>

\[A^T A \hat{x} = A^T b\]

<hr />

<h2 id="54-er-mindste-kvadraters-l√∏sninger-entydige">54. Er mindste kvadraters l√∏sninger entydige?</h2>

<p>Ikke altid. De er entydige hvis og kun hvis $A$ har fuld rang, eller √¶kvivalent hvis $A^T A$ er inverterbar.</p>

<hr />

<h2 id="55-hvorn√•r-giver-det-mening-at-bruge-mindste-kvadraters-metoden">55. Hvorn√•r giver det mening at bruge mindste kvadraters metoden?</h2>

<p>N√•r man ikke har nogen grund til at tro, eller ikke er sikker p√•, at det oprindelige system $Ax = b$ er konsistent.</p>

<hr />

<h2 id="56-hvad-hvis-jeg-udregner-en-mindste-kvadraters-l√∏sning-hatx-til-et-system-ax--b-som-faktisk-er-konsistent">56. Hvad, hvis jeg udregner en mindste kvadraters l√∏sning $\hat{x}$ til et system $Ax = b$, som faktisk er konsistent?</h2>

<p>S√• er enhver mindste kvadraters l√∏sning $\hat{x}$ ogs√• en almindelig l√∏sning til $Ax = b$, dvs.</p>

\[A \hat{x} = b\]

<hr />

<h2 id="57-hvad-er-et-line√¶rt-programmeringsproblem-p√•-kanonisk-form">57. Hvad er et line√¶rt programmeringsproblem p√• kanonisk form?</h2>

<p>Maksimer</p>

\[c^T x\]

<p>under bibetingelserne</p>

\[A x \leq b, \quad x \geq 0\]

<hr />

<h2 id="58-hvordan-kan-bibetingelser-som-a_it-x-geq-b_i-eller-a_it-x--b_i-omskrives-i-et-line√¶rt-programmeringsproblem-p√•-kanonisk-form">58. Hvordan kan bibetingelser som $a_i^T x \geq b_i$ eller $a_i^T x = b_i$ omskrives i et line√¶rt programmeringsproblem p√• kanonisk form?</h2>

<ul>
  <li>$a_i^T x \geq b_i$ omskrives som $-a_i^T x \leq -b_i$</li>
  <li>$a_i^T x = b_i$ omskrives som to uligheder: $a_i^T x \leq b_i$ og $a_i^T x \geq b_i$</li>
</ul>

<hr />

<h2 id="59-hvad-hvis-man-vil-minimere-ct-x-kan-man-stadigv√¶k-opstille-et-line√¶rt-programmeringsproblem-p√•-kanonisk-form">59. Hvad, hvis man vil minimere $c^T x$: kan man stadigv√¶k opstille et line√¶rt programmeringsproblem p√• kanonisk form?</h2>

<p>Ja, minimere $c^T x$ kan omskrives til maksimering af $-c^T x$.</p>

<hr />

<h2 id="60-kender-du-to-situationer-hvor-man-ikke-finder-en-optimal-l√∏sning-til-et-line√¶rt-programmeringsproblem">60. Kender du to situationer, hvor man ikke finder en optimal l√∏sning til et line√¶rt programmeringsproblem?</h2>

<ol>
  <li>Feasibility set $F$ er tomt (ingen l√∏sninger).</li>
  <li>$c^T x$ er ubegr√¶nset p√• $F$ (ingen maksimum).</li>
</ol>

<hr />

<h2 id="61-hvis-f-ikke-er-tom-og-ct-x-er-begr√¶nset-p√•-f-hvor-i-f-ligger-de-optimale-l√∏sninger">61. Hvis $F$ ikke er tom og $c^T x$ er begr√¶nset p√• $F$, hvor i $F$ ligger de optimale l√∏sninger?</h2>

<p>Mindst √©n optimal l√∏sning findes i et ekstremalpunkt (hj√∏rnepunkt) af den konvekse m√¶ngde $F$</p>

<hr />

<h2 id="62-skitser-m√¶ngden-f-af-alle-vektorer-x--x_1-x_2-der-opfylder">62. Skitser m√¶ngden $F$ af alle vektorer $x = (x_1, x_2)$, der opfylder</h2>

\[0 \leq x_1 \leq 2, \quad 0 \leq x_2 \leq 2, \quad x_1 + x_2 \leq 3\]

<p>Hvilke punkter i $F$ ville du afpr√∏ve som mulige optimale l√∏sninger i den grafiske metode?</p>

<p>Svar: De mulige optimale l√∏sninger ligger i hj√∏rnerne af $F$:</p>

\[(0,0), (2,0), (0,2), (2,1), (1,2)\]

<hr />

<h2 id="63-hvordan-opstiller-man-en-initial-simplextableau-begyndelsessimplextableau-for-et-line√¶rt-programmeringsproblem-p√•-kanonisk-form">63. Hvordan opstiller man en initial simplextableau (begyndelsessimplextableau) for et line√¶rt programmeringsproblem p√• kanonisk form?</h2>

<ul>
  <li>Tilf√∏j slack-variabler for at omskrive uligheder til ligheder.</li>
  <li>Antal slack-variabler svarer til antallet af uligheder.</li>
  <li>Objektfunktionen tilf√∏jes som sidste r√¶kke i tableau.</li>
</ul>

<hr />

<h2 id="64-beskriv-algoritmen-i-simplexmetoden-i-tilf√¶lde-hvor-alle-indgange-i-b-er-positive">64. Beskriv algoritmen i simplexmetoden i tilf√¶lde, hvor alle indgange i $b$ er positive!</h2>

<ul>
  <li>V√¶lg indgangsvariabel (variabel med mest negativ koefficient i objektfunktionen).</li>
  <li>Bestem udgangsvariabel via minimum ratio test.</li>
  <li>Udf√∏r pivotering.</li>
  <li>Gentag indtil ingen negative koefficienter i objektfunktionen.</li>
  <li>Optimal l√∏sning afl√¶ses i tableau.</li>
</ul>

<hr />

<h2 id="65-kan-man-g√∏re-noget-hvis-b-har-en-eller-flere-negative-indgange">65. Kan man g√∏re noget, hvis $b$ har en eller flere negative indgange?</h2>

<p>Ja, man kan bruge en ‚Äúbig M‚Äù metode eller to-fase simplex til at starte med en basisl√∏sning.</p>

<hr />

<h2 id="66-hvad-er-det-duale-problem-til-et-line√¶rt-programmeringsproblem-p√•-kanonisk-form">66. Hvad er det duale problem til et line√¶rt programmeringsproblem p√• kanonisk form?</h2>

<p>Hvis primalproblemet er</p>

\[\max c^T x \quad \text{under} \quad A x \leq b, \quad x \geq 0\]

<p>s√• er det duale problem</p>

\[\min b^T y \quad \text{under} \quad A^T y \geq c, \quad y \geq 0\]

<hr />

<h2 id="67-hvad-er-udsagnet-af-dualitetss√¶tningen-om-line√¶re-programmeringsproblemer-samt-deres-duale-problemer">67. Hvad er udsagnet af dualitetss√¶tningen om line√¶re programmeringsproblemer samt deres duale problemer?</h2>

<p>Hvis primalproblemet har en optimal l√∏sning, s√• har dualproblemet ogs√• en optimal l√∏sning, og deres optimale v√¶rdier er ens:</p>

\[\max c^T x = \min b^T y\]

<p>Desuden, hvis enten primal eller dual har ubegr√¶nset l√∏sning, s√• er det modsatte problem ufeasible.</p>
